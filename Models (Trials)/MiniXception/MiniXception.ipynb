{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniXception.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YNBRPaTq8ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMMjxOUbGi10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Conv2D, Activation, BatchNormalization, SeparableConv2D,MaxPooling2D,GlobalAveragePooling2D\n",
        "from keras import Model\n",
        "from keras import layers\n",
        "from keras.regularizers import l2 \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import pickle\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74FH_FvAGmZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
        "  regularization = l2(l2_regularization)\n",
        "  \n",
        "  img_input = Input(input_shape)\n",
        "  x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,use_bias=False)(img_input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  \n",
        "  residual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
        "  residual = BatchNormalization()(residual)\n",
        "  x = SeparableConv2D(16, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = SeparableConv2D(16, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "  x = layers.add([x, residual]) \n",
        "\n",
        "  residual = Conv2D(32, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x) \n",
        "  residual = BatchNormalization()(residual)\n",
        "  x = SeparableConv2D(32, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = SeparableConv2D(32, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "  x = layers.add([x, residual]) \n",
        "\n",
        "  residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
        "  residual = BatchNormalization()(residual)\n",
        "  x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = SeparableConv2D(64, (3, 3), padding='same', kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "  x = layers.add([x, residual])\n",
        "\n",
        "  residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
        "  residual = BatchNormalization()(residual)\n",
        "\n",
        "  x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = SeparableConv2D(128, (3, 3), padding='same', kernel_regularizer=regularization,use_bias=False)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "  x = layers.add([x, residual])\n",
        "  x = Conv2D(num_classes, (3, 3),\n",
        "             #kernel_regularizer=regularization, \n",
        "             padding='same')(x)\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "  output = Activation('softmax',name='predictions')(x)\n",
        "  model = Model(img_input, output)\n",
        "  return model  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcJVxeIQIT0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ORe5zv2wvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "data = pd.read_csv('fer2013.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzexldgCn8gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as  np\n",
        "\n",
        "train = data[data.Usage == \"Training\"]\n",
        "test = data[data.Usage == \"PrivateTest\"]\n",
        "test_pub = data[data.Usage == \"PublicTest\"]\n",
        "\n",
        "\n",
        "train_labels = train[\"emotion\"].values\n",
        "test_labels = test[\"emotion\"].values\n",
        "test_pub_labels = test_pub[\"emotion\"].values\n",
        "\n",
        "train_ds = train[\"pixels\"].values\n",
        "test_ds = test[\"pixels\"].values\n",
        "test_pub_ds = test_pub[\"pixels\"].values\n",
        "\n",
        "train_ds_list = list()\n",
        "test_ds_list = list()\n",
        "test_pub_ds_list = list()\n",
        "\n",
        "for i in range(28709):\n",
        "  train_ds_list.append(np.array(train_ds[i].split(\" \")))\n",
        "  train_ds_list[i] = train_ds_list[i].astype(np.float)\n",
        "\n",
        "for i in range(3589):\n",
        "  test_ds_list.append(np.array(test_ds[i].split(\" \")))\n",
        "  test_ds_list[i] = test_ds_list[i].astype(np.float)\n",
        "\n",
        "for i in range(3589):\n",
        "  test_pub_ds_list.append(np.array(test_pub_ds[i].split(\" \")))\n",
        "  test_pub_ds_list[i] = test_pub_ds_list[i].astype(np.float)\n",
        "\n",
        "train_np = np.array(train_ds_list)\n",
        "test_np = np.array(test_ds_list)\n",
        "test_pub_np = np.array(test_pub_ds_list)\n",
        "\n",
        "\n",
        "\n",
        "train_np = train_np.reshape(train_np.shape[0], 48, 48, 1)\n",
        "test_np = test_np.reshape(test_np.shape[0],48, 48, 1)\n",
        "test_pub_np = test_pub_np.reshape(test_pub_np.shape[0], 48, 48, 1)\n",
        "\n",
        "train_labels = np_utils.to_categorical(train_labels, 7)\n",
        "test_labels = np_utils.to_categorical(test_labels, 7)\n",
        "test_pub_labels = np_utils.to_categorical(test_pub_labels, 7) \n",
        "\n",
        "patience = 50\n",
        "batch_size = 64\n",
        "num_epochs = 60\n",
        "\n",
        "\n",
        "train_big = np.vstack([train_np, test_pub_np])\n",
        "train_label_big = np.concatenate([train_labels, test_pub_labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfT4NBp8oMQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
        "                                  patience=int(patience/4), verbose=1)\n",
        "model_names ='weights_mini_xception.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
        "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
        "                                                    save_best_only=True)\n",
        "callbacks = [model_checkpoint, early_stop, reduce_lr]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdAhxqS7oQcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "eedd057b-11dd-4e0a-f7c3-9646b3aab274"
      },
      "source": [
        "model = mini_XCEPTION((48, 48, 1), 7)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_lnfcGnoSbZ",
        "colab_type": "code",
        "outputId": "0afc47bf-679f-4884-b0b7-7d784ed3c56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(data_generator.flow(train_big, train_label_big,\n",
        "                                            batch_size),\n",
        "                        steps_per_epoch=len(train_big) / batch_size,\n",
        "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
        "                        validation_data=[test_np,test_labels])\n",
        "model.save_weights(\"weights_60_epochs_xception.hdf5\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "505/504 [==============================] - 29s 57ms/step - loss: 1.7791 - acc: 0.3264 - val_loss: 1.6779 - val_acc: 0.3720\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.67790, saving model to weights_mini_xception.01-0.37.hdf5\n",
            "Epoch 2/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.5394 - acc: 0.4246 - val_loss: 1.5032 - val_acc: 0.4372\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.67790 to 1.50322, saving model to weights_mini_xception.02-0.44.hdf5\n",
            "Epoch 3/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.4187 - acc: 0.4710 - val_loss: 1.4955 - val_acc: 0.4820\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.50322 to 1.49547, saving model to weights_mini_xception.03-0.48.hdf5\n",
            "Epoch 4/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.3306 - acc: 0.5008 - val_loss: 1.4028 - val_acc: 0.4879\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.49547 to 1.40278, saving model to weights_mini_xception.04-0.49.hdf5\n",
            "Epoch 5/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.2804 - acc: 0.5222 - val_loss: 1.2577 - val_acc: 0.5222\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.40278 to 1.25766, saving model to weights_mini_xception.05-0.52.hdf5\n",
            "Epoch 6/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.2405 - acc: 0.5343 - val_loss: 1.3060 - val_acc: 0.5174\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.25766\n",
            "Epoch 7/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.2174 - acc: 0.5433 - val_loss: 1.2548 - val_acc: 0.5313\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.25766 to 1.25480, saving model to weights_mini_xception.07-0.53.hdf5\n",
            "Epoch 8/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.1911 - acc: 0.5530 - val_loss: 1.2102 - val_acc: 0.5539\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.25480 to 1.21022, saving model to weights_mini_xception.08-0.55.hdf5\n",
            "Epoch 9/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.1759 - acc: 0.5607 - val_loss: 1.1694 - val_acc: 0.5676\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.21022 to 1.16943, saving model to weights_mini_xception.09-0.57.hdf5\n",
            "Epoch 10/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.1606 - acc: 0.5647 - val_loss: 1.3831 - val_acc: 0.4787\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.16943\n",
            "Epoch 11/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.1404 - acc: 0.5732 - val_loss: 1.2425 - val_acc: 0.5567\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.16943\n",
            "Epoch 12/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.1266 - acc: 0.5785 - val_loss: 1.1637 - val_acc: 0.5575\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.16943 to 1.16372, saving model to weights_mini_xception.12-0.56.hdf5\n",
            "Epoch 13/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.1090 - acc: 0.5871 - val_loss: 1.1884 - val_acc: 0.5592\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.16372\n",
            "Epoch 14/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.1036 - acc: 0.5846 - val_loss: 1.1281 - val_acc: 0.5821\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.16372 to 1.12815, saving model to weights_mini_xception.14-0.58.hdf5\n",
            "Epoch 15/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0935 - acc: 0.5886 - val_loss: 1.1354 - val_acc: 0.5737\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.12815\n",
            "Epoch 16/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0820 - acc: 0.5921 - val_loss: 1.1852 - val_acc: 0.5598\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.12815\n",
            "Epoch 17/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0776 - acc: 0.5966 - val_loss: 1.1434 - val_acc: 0.5687\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.12815\n",
            "Epoch 18/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0628 - acc: 0.6018 - val_loss: 1.0986 - val_acc: 0.5896\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.12815 to 1.09856, saving model to weights_mini_xception.18-0.59.hdf5\n",
            "Epoch 19/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0608 - acc: 0.6047 - val_loss: 1.1241 - val_acc: 0.5907\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.09856\n",
            "Epoch 20/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0544 - acc: 0.6033 - val_loss: 1.1700 - val_acc: 0.5795\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.09856\n",
            "Epoch 21/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0426 - acc: 0.6109 - val_loss: 1.0857 - val_acc: 0.5910\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.09856 to 1.08570, saving model to weights_mini_xception.21-0.59.hdf5\n",
            "Epoch 22/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0414 - acc: 0.6111 - val_loss: 1.0581 - val_acc: 0.6021\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.08570 to 1.05810, saving model to weights_mini_xception.22-0.60.hdf5\n",
            "Epoch 23/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0287 - acc: 0.6173 - val_loss: 1.0633 - val_acc: 0.6046\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.05810\n",
            "Epoch 24/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0257 - acc: 0.6164 - val_loss: 1.0868 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.05810\n",
            "Epoch 25/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.0198 - acc: 0.6202 - val_loss: 1.1167 - val_acc: 0.5865\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.05810\n",
            "Epoch 26/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.0194 - acc: 0.6183 - val_loss: 1.0759 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.05810\n",
            "Epoch 27/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.0113 - acc: 0.6211 - val_loss: 1.1021 - val_acc: 0.6035\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.05810\n",
            "Epoch 28/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0111 - acc: 0.6208 - val_loss: 1.0988 - val_acc: 0.5915\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.05810\n",
            "Epoch 29/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 1.0020 - acc: 0.6248 - val_loss: 1.0772 - val_acc: 0.6155\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.05810\n",
            "Epoch 30/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 1.0004 - acc: 0.6258 - val_loss: 1.0806 - val_acc: 0.6032\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.05810\n",
            "Epoch 31/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9942 - acc: 0.6282 - val_loss: 1.0403 - val_acc: 0.6180\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.05810 to 1.04034, saving model to weights_mini_xception.31-0.62.hdf5\n",
            "Epoch 32/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9912 - acc: 0.6318 - val_loss: 1.0157 - val_acc: 0.6241\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.04034 to 1.01567, saving model to weights_mini_xception.32-0.62.hdf5\n",
            "Epoch 33/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9893 - acc: 0.6307 - val_loss: 1.0460 - val_acc: 0.6099\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.01567\n",
            "Epoch 34/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9875 - acc: 0.6295 - val_loss: 1.0261 - val_acc: 0.6222\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.01567\n",
            "Epoch 35/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9838 - acc: 0.6319 - val_loss: 1.0534 - val_acc: 0.6096\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.01567\n",
            "Epoch 36/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9735 - acc: 0.6361 - val_loss: 1.0992 - val_acc: 0.5954\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.01567\n",
            "Epoch 37/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9748 - acc: 0.6365 - val_loss: 1.1191 - val_acc: 0.5918\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.01567\n",
            "Epoch 38/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9706 - acc: 0.6387 - val_loss: 1.0677 - val_acc: 0.6055\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.01567\n",
            "Epoch 39/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.9687 - acc: 0.6401 - val_loss: 1.0787 - val_acc: 0.6160\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.01567\n",
            "Epoch 40/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9653 - acc: 0.6403 - val_loss: 1.0696 - val_acc: 0.6049\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.01567\n",
            "Epoch 41/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9625 - acc: 0.6411 - val_loss: 1.0538 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.01567\n",
            "Epoch 42/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.9550 - acc: 0.6421 - val_loss: 1.0766 - val_acc: 0.6121\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.01567\n",
            "Epoch 43/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.9495 - acc: 0.6464 - val_loss: 1.0275 - val_acc: 0.6275\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.01567\n",
            "Epoch 44/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9558 - acc: 0.6435 - val_loss: 1.0391 - val_acc: 0.6211\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.01567\n",
            "\n",
            "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 45/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.9118 - acc: 0.6584 - val_loss: 0.9864 - val_acc: 0.6378\n",
            "\n",
            "Epoch 00045: val_loss improved from 1.01567 to 0.98635, saving model to weights_mini_xception.45-0.64.hdf5\n",
            "Epoch 46/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.8941 - acc: 0.6678 - val_loss: 0.9790 - val_acc: 0.6425\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.98635 to 0.97898, saving model to weights_mini_xception.46-0.64.hdf5\n",
            "Epoch 47/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.8936 - acc: 0.6668 - val_loss: 0.9794 - val_acc: 0.6422\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.97898\n",
            "Epoch 48/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.8858 - acc: 0.6716 - val_loss: 0.9795 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.97898\n",
            "Epoch 49/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8836 - acc: 0.6689 - val_loss: 0.9792 - val_acc: 0.6450\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.97898\n",
            "Epoch 50/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8858 - acc: 0.6714 - val_loss: 0.9824 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.97898\n",
            "Epoch 51/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.8789 - acc: 0.6738 - val_loss: 0.9776 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.97898 to 0.97760, saving model to weights_mini_xception.51-0.65.hdf5\n",
            "Epoch 52/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8820 - acc: 0.6710 - val_loss: 0.9750 - val_acc: 0.6450\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.97760 to 0.97505, saving model to weights_mini_xception.52-0.65.hdf5\n",
            "Epoch 53/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8732 - acc: 0.6741 - val_loss: 0.9774 - val_acc: 0.6450\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.97505\n",
            "Epoch 54/60\n",
            "505/504 [==============================] - 22s 44ms/step - loss: 0.8749 - acc: 0.6741 - val_loss: 0.9749 - val_acc: 0.6461\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.97505 to 0.97487, saving model to weights_mini_xception.54-0.65.hdf5\n",
            "Epoch 55/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8718 - acc: 0.6751 - val_loss: 0.9828 - val_acc: 0.6383\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.97487\n",
            "Epoch 56/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8717 - acc: 0.6755 - val_loss: 0.9731 - val_acc: 0.6467\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.97487 to 0.97308, saving model to weights_mini_xception.56-0.65.hdf5\n",
            "Epoch 57/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8687 - acc: 0.6770 - val_loss: 0.9823 - val_acc: 0.6478\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.97308\n",
            "Epoch 58/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8674 - acc: 0.6816 - val_loss: 0.9754 - val_acc: 0.6489\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.97308\n",
            "Epoch 59/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8670 - acc: 0.6790 - val_loss: 0.9895 - val_acc: 0.6420\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.97308\n",
            "Epoch 60/60\n",
            "505/504 [==============================] - 22s 43ms/step - loss: 0.8669 - acc: 0.6742 - val_loss: 0.9774 - val_acc: 0.6467\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.97308\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}