{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet18For EmotionDetection.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GVfmDN2sH6Xl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"b4547815-05ef-4ebb-85a0-c2158b3076b9","executionInfo":{"status":"ok","timestamp":1575066043540,"user_tz":-120,"elapsed":3638,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["\n","import numpy as np\n","from keras import layers\n","from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n","from keras.models import Model, load_model\n","from keras.preprocessing import image\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from keras.applications.imagenet_utils import preprocess_input\n","import pydot\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.utils import plot_model\n","import pandas as pd\n","#from resnets_utils import *\n","from keras.initializers import glorot_uniform\n","import scipy.misc\n","from matplotlib.pyplot import imshow\n","%matplotlib inline\n","\n","import keras.backend as K\n","K.set_image_data_format('channels_last')\n","K.set_learning_phase(1)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k0VPG955IGol","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: identity_block\n","\n","def identity_block(X, f, filters, stage, block):\n","    \"\"\"\n","    Implementation of the identity block as defined in Figure 4\n","    \n","    Arguments:\n","    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    f -- integer, specifying the shape of the middle CONV's window for the main path\n","    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n","    stage -- integer, used to name the layers, depending on their position in the network\n","    block -- string/character, used to name the layers, depending on their position in the network\n","    \n","    Returns:\n","    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n","    \"\"\"\n","    \n","    # defining name basis\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # Retrieve Filters\n","    F1, F2, F3 = filters\n","    \n","    # Save the input value. You'll need this later to add back to the main path. \n","    X_shortcut = X\n","    print\n","    # First component of main path\n","    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","    X = Activation('relu')(X)\n","    \n","    ### START CODE HERE ###\n","    \n","    # Second component of main path (≈3 lines)\n","    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n","    X = Activation('relu')(X)\n","    # Third component of main path (≈2 lines)\n","    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n","    X = Add()([X, X_shortcut])\n","    X = Activation('relu')(X)\n","    \n","    ### END CODE HERE ###\n","    \n","    return X"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"er24cGe9IPb3","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: convolutional_block\n","\n","def convolutional_block(X, f, filters, stage, block, s = 2):\n","    \"\"\"\n","    Implementation of the convolutional block as defined in Figure 4\n","    \n","    Arguments:\n","    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    f -- integer, specifying the shape of the middle CONV's window for the main path\n","    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n","    stage -- integer, used to name the layers, depending on their position in the network\n","    block -- string/character, used to name the layers, depending on their position in the network\n","    s -- Integer, specifying the stride to be used\n","    \n","    Returns:\n","    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n","    \"\"\"\n","    \n","    # defining name basis\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # Retrieve Filters\n","    F1, F2, F3 = filters\n","    \n","    # Save the input value\n","    X_shortcut = X\n","\n","\n","    ##### MAIN PATH #####\n","    # First component of main path \n","    X = Conv2D(F1, (1, 1), strides = (s,s),padding='valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","    X = Activation('relu')(X)\n","    \n","    ### START CODE HERE ###\n","\n","    # Second component of main path (≈3 lines)\n","    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n","    X = Activation('relu')(X)\n","\n","    # Third component of main path (≈2 lines)\n","    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n","\n","    ##### SHORTCUT PATH #### (≈2 lines)\n","    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n","    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n","    X = Add()([X, X_shortcut])\n","    X = Activation('relu')(X)\n","\n","    ### END CODE HERE ###\n","    \n","    return X"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOGF-e3_RLXe","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: ResNet50\n","\n","def ResNet18(input_shape = (64, 64, 3), classes = 6):\n","    \"\"\"\n","    Implementation of the popular ResNet50 the following architecture:\n","    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n","    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n","\n","    Arguments:\n","    input_shape -- shape of the images of the dataset\n","    classes -- integer, number of classes\n","\n","    Returns:\n","    model -- a Model() instance in Keras\n","    \"\"\"\n","    \n","    # Define the input as a tensor with shape input_shape\n","    X_input = Input(input_shape)\n","\n","    \n","    # Zero-Padding\n","    X = ZeroPadding2D((3, 3))(X_input)\n","    \n","    # Stage 1\n","    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n","\n","    # Stage 2\n","    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n","    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n","\n","    ### START CODE HERE ###\n","\n","    # Stage 3 (≈4 lines)\n","    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n","    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n","\n","\n","    # Stage 4 (≈6 lines)\n","    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n","    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n","\n","\n","    # Stage 5 (≈3 lines)\n","    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n","    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n","\n","    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n","    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n","    \n","    ### END CODE HERE ###\n","\n","    # output layer\n","    X = Flatten()(X)\n","    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n","    \n","    \n","    # Create model\n","    model = Model(inputs = X_input, outputs = X, name='ResNet18')\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ipnft1mdNjjJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":427},"outputId":"2270c180-e86e-4796-9cc7-409c31d51de7","executionInfo":{"status":"ok","timestamp":1575066046404,"user_tz":-120,"elapsed":6440,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["model = ResNet18(input_shape = (48, 48, 3), classes = 7)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tMm-y6PeNoku","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"a9feef33-0875-41ad-e3a3-f12564761ff0","executionInfo":{"status":"ok","timestamp":1575066046405,"user_tz":-120,"elapsed":6430,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J-Keb6QdNuFK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"717c82ef-9597-4f2d-8e73-556dd5f1f117","executionInfo":{"status":"ok","timestamp":1575066046406,"user_tz":-120,"elapsed":6412,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kT3y4Az4N2DV","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.upload()  #this will prompt you to upload the kaggle.json\n","!pip install -q kaggle --upgrade\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!ls ~/.kaggle\n","!chmod 600 /root/.kaggle/kaggle.json  # set permission\n","!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kd9posLGOApO","colab_type":"code","colab":{}},"source":["!tar -xf \"/content/fer2013.tar.gz\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYNngLJCOK6J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"257648a6-863c-4117-8271-5db9c8dff8df","executionInfo":{"status":"ok","timestamp":1575066110396,"user_tz":-120,"elapsed":42201,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["from keras.utils import np_utils\n","data = pd.read_csv('/content/fer2013/fer2013.csv')\n","train = data[data.Usage == \"Training\"]\n","test = data[data.Usage == \"PrivateTest\"]\n","test_pub = data[data.Usage == \"PublicTest\"]\n","\n","\n","train_labels = train[\"emotion\"].values\n","test_labels = test[\"emotion\"].values\n","test_pub_labels = test_pub[\"emotion\"].values\n","\n","train_ds = train[\"pixels\"].values\n","test_ds = test[\"pixels\"].values\n","test_pub_ds = test_pub[\"pixels\"].values\n","\n","train_ds_list = list()\n","test_ds_list = list()\n","test_pub_ds_list = list()\n","\n","for i in range(28709):\n","  train_ds_list.append(np.array(train_ds[i].split(\" \")))\n","  train_ds_list[i] = train_ds_list[i].astype(np.float)\n","\n","for i in range(3589):\n","  test_ds_list.append(np.array(test_ds[i].split(\" \")))\n","  test_ds_list[i] = test_ds_list[i].astype(np.float)\n","\n","for i in range(3589):\n","  test_pub_ds_list.append(np.array(test_pub_ds[i].split(\" \")))\n","  test_pub_ds_list[i] = test_pub_ds_list[i].astype(np.float)\n","\n","train_np = np.array(train_ds_list)\n","test_np = np.array(test_ds_list)\n","test_pub_np = np.array(test_pub_ds_list)\n","\n","\n","\n","train_np = train_np.reshape(train_np.shape[0], 48, 48, 1)\n","test_np = test_np.reshape(test_np.shape[0],48, 48, 1)\n","test_pub_np = test_pub_np.reshape(test_pub_np.shape[0], 48, 48, 1)\n","\n","train_np = np.repeat(train_np , 3, axis=3)\n","test_np = np.repeat(test_np , 3, axis=3)\n","test_pub_np = np.repeat(test_pub_np , 3, axis=3)\n","print(train_np.shape)\n","\n","train_labels = np_utils.to_categorical(train_labels, 7)\n","test_labels = np_utils.to_categorical(test_labels, 7)\n","test_pub_labels = np_utils.to_categorical(test_pub_labels, 7) \n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(28709, 48, 48, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KecmwQqDOPa3","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","# parameters\n","num_epochs = 100\n","verbose = 1\n","batch_size = 64\n","\n","\n","train_big = np.vstack([train_np, test_pub_np])\n","train_label_big = np.concatenate([train_labels, test_pub_labels])\n","# data generator\n","data_generator = ImageDataGenerator(rescale = 1./255.,\n","                                   rotation_range = 40,\n","                                   width_shift_range = 0.2,\n","                                   height_shift_range = 0.2,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVlrsa80O9tQ","colab_type":"code","colab":{}},"source":["model.save('Resnet18_model.hdf5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE3rMuRwPCUG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"889ef833-eb11-4510-fc0f-7830c40caf72","executionInfo":{"status":"ok","timestamp":1575068741388,"user_tz":-120,"elapsed":2524308,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["model.fit_generator(data_generator.flow(train_big, train_label_big),\n","                        steps_per_epoch=len(train_big) / batch_size,\n","                        epochs=num_epochs, verbose=1, \n","                    validation_data=data_generator.flow(test_np, test_labels)\n",")"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","505/504 [==============================] - 27s 54ms/step - loss: 1.8843 - acc: 0.2394 - val_loss: 1.8680 - val_acc: 0.2379\n","Epoch 2/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.8989 - acc: 0.2406 - val_loss: 1.9603 - val_acc: 0.2438\n","Epoch 3/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.8939 - acc: 0.2433 - val_loss: 1.8798 - val_acc: 0.2416\n","Epoch 4/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.8555 - acc: 0.2405 - val_loss: 1.8373 - val_acc: 0.2491\n","Epoch 5/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.8399 - acc: 0.2452 - val_loss: 1.8183 - val_acc: 0.2488\n","Epoch 6/100\n","505/504 [==============================] - 27s 52ms/step - loss: 1.8044 - acc: 0.2533 - val_loss: 1.7854 - val_acc: 0.2664\n","Epoch 7/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7901 - acc: 0.2598 - val_loss: 1.8006 - val_acc: 0.2488\n","Epoch 8/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7792 - acc: 0.2597 - val_loss: 1.7854 - val_acc: 0.2547\n","Epoch 9/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7710 - acc: 0.2695 - val_loss: 1.7728 - val_acc: 0.2614\n","Epoch 10/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7694 - acc: 0.2712 - val_loss: 1.7922 - val_acc: 0.2602\n","Epoch 11/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.7654 - acc: 0.2683 - val_loss: 1.7610 - val_acc: 0.2733\n","Epoch 12/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.7547 - acc: 0.2737 - val_loss: 1.7450 - val_acc: 0.2781\n","Epoch 13/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.7289 - acc: 0.2966 - val_loss: 1.7486 - val_acc: 0.2823\n","Epoch 14/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7332 - acc: 0.2884 - val_loss: 1.7233 - val_acc: 0.2965\n","Epoch 15/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.7186 - acc: 0.3000 - val_loss: 1.7162 - val_acc: 0.3012\n","Epoch 16/100\n","505/504 [==============================] - 27s 53ms/step - loss: 1.6829 - acc: 0.3206 - val_loss: 1.6663 - val_acc: 0.3293\n","Epoch 17/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.6535 - acc: 0.3470 - val_loss: 1.6559 - val_acc: 0.3380\n","Epoch 18/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.6487 - acc: 0.3402 - val_loss: 1.6071 - val_acc: 0.3720\n","Epoch 19/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.6028 - acc: 0.3694 - val_loss: 1.6030 - val_acc: 0.3664\n","Epoch 20/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.5882 - acc: 0.3732 - val_loss: 1.5687 - val_acc: 0.3929\n","Epoch 21/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.5587 - acc: 0.3871 - val_loss: 1.5272 - val_acc: 0.3979\n","Epoch 22/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.5404 - acc: 0.3936 - val_loss: 1.5253 - val_acc: 0.4082\n","Epoch 23/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.5095 - acc: 0.4093 - val_loss: 1.5006 - val_acc: 0.4160\n","Epoch 24/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.4859 - acc: 0.4218 - val_loss: 1.4871 - val_acc: 0.4266\n","Epoch 25/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.4656 - acc: 0.4327 - val_loss: 1.4714 - val_acc: 0.4291\n","Epoch 26/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.4505 - acc: 0.4408 - val_loss: 1.4571 - val_acc: 0.4296\n","Epoch 27/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.4301 - acc: 0.4451 - val_loss: 1.4141 - val_acc: 0.4553\n","Epoch 28/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.4280 - acc: 0.4452 - val_loss: 1.4123 - val_acc: 0.4472\n","Epoch 29/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3982 - acc: 0.4609 - val_loss: 1.3886 - val_acc: 0.4706\n","Epoch 30/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.4032 - acc: 0.4601 - val_loss: 1.3748 - val_acc: 0.4714\n","Epoch 31/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3713 - acc: 0.4722 - val_loss: 1.3528 - val_acc: 0.4784\n","Epoch 32/100\n","505/504 [==============================] - 26s 52ms/step - loss: 1.3689 - acc: 0.4749 - val_loss: 1.3861 - val_acc: 0.4642\n","Epoch 33/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3434 - acc: 0.4852 - val_loss: 1.3725 - val_acc: 0.4687\n","Epoch 34/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3488 - acc: 0.4770 - val_loss: 1.3459 - val_acc: 0.4882\n","Epoch 35/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3418 - acc: 0.4868 - val_loss: 1.3057 - val_acc: 0.4946\n","Epoch 36/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3208 - acc: 0.4933 - val_loss: 1.3221 - val_acc: 0.4929\n","Epoch 37/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3145 - acc: 0.5008 - val_loss: 1.3031 - val_acc: 0.5004\n","Epoch 38/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.3225 - acc: 0.4958 - val_loss: 1.3012 - val_acc: 0.4990\n","Epoch 39/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2948 - acc: 0.5011 - val_loss: 1.3079 - val_acc: 0.5007\n","Epoch 40/100\n","505/504 [==============================] - 26s 51ms/step - loss: 1.2904 - acc: 0.5089 - val_loss: 1.2936 - val_acc: 0.5046\n","Epoch 41/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2953 - acc: 0.5017 - val_loss: 1.2860 - val_acc: 0.4993\n","Epoch 42/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2682 - acc: 0.5156 - val_loss: 1.2790 - val_acc: 0.5155\n","Epoch 43/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2548 - acc: 0.5257 - val_loss: 1.2525 - val_acc: 0.5261\n","Epoch 44/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2642 - acc: 0.5165 - val_loss: 1.2771 - val_acc: 0.5052\n","Epoch 45/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2421 - acc: 0.5278 - val_loss: 1.2542 - val_acc: 0.5146\n","Epoch 46/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2605 - acc: 0.5218 - val_loss: 1.2279 - val_acc: 0.5227\n","Epoch 47/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2437 - acc: 0.5275 - val_loss: 1.2454 - val_acc: 0.5280\n","Epoch 48/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2302 - acc: 0.5345 - val_loss: 1.2485 - val_acc: 0.5258\n","Epoch 49/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2253 - acc: 0.5336 - val_loss: 1.2300 - val_acc: 0.5322\n","Epoch 50/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.2246 - acc: 0.5327 - val_loss: 1.2603 - val_acc: 0.5280\n","Epoch 51/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.2264 - acc: 0.5348 - val_loss: 1.2098 - val_acc: 0.5358\n","Epoch 52/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.2150 - acc: 0.5352 - val_loss: 1.2071 - val_acc: 0.5472\n","Epoch 53/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.2074 - acc: 0.5362 - val_loss: 1.2133 - val_acc: 0.5369\n","Epoch 54/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.2014 - acc: 0.5393 - val_loss: 1.2108 - val_acc: 0.5464\n","Epoch 55/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1976 - acc: 0.5473 - val_loss: 1.2260 - val_acc: 0.5355\n","Epoch 56/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1975 - acc: 0.5464 - val_loss: 1.2038 - val_acc: 0.5344\n","Epoch 57/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1925 - acc: 0.5479 - val_loss: 1.2060 - val_acc: 0.5389\n","Epoch 58/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1974 - acc: 0.5456 - val_loss: 1.1877 - val_acc: 0.5444\n","Epoch 59/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1837 - acc: 0.5545 - val_loss: 1.1923 - val_acc: 0.5430\n","Epoch 60/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1759 - acc: 0.5594 - val_loss: 1.1924 - val_acc: 0.5531\n","Epoch 61/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1736 - acc: 0.5598 - val_loss: 1.2005 - val_acc: 0.5430\n","Epoch 62/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1774 - acc: 0.5498 - val_loss: 1.1986 - val_acc: 0.5397\n","Epoch 63/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1660 - acc: 0.5604 - val_loss: 1.1720 - val_acc: 0.5495\n","Epoch 64/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1707 - acc: 0.5585 - val_loss: 1.1736 - val_acc: 0.5531\n","Epoch 65/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1597 - acc: 0.5655 - val_loss: 1.1599 - val_acc: 0.5508\n","Epoch 66/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1615 - acc: 0.5582 - val_loss: 1.1696 - val_acc: 0.5612\n","Epoch 67/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1715 - acc: 0.5609 - val_loss: 1.1873 - val_acc: 0.5520\n","Epoch 68/100\n","505/504 [==============================] - 24s 49ms/step - loss: 1.1411 - acc: 0.5677 - val_loss: 1.1519 - val_acc: 0.5687\n","Epoch 69/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1534 - acc: 0.5623 - val_loss: 1.1777 - val_acc: 0.5573\n","Epoch 70/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1489 - acc: 0.5704 - val_loss: 1.1601 - val_acc: 0.5673\n","Epoch 71/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1385 - acc: 0.5722 - val_loss: 1.1358 - val_acc: 0.5712\n","Epoch 72/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1486 - acc: 0.5620 - val_loss: 1.1399 - val_acc: 0.5687\n","Epoch 73/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1364 - acc: 0.5713 - val_loss: 1.1557 - val_acc: 0.5639\n","Epoch 74/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1279 - acc: 0.5692 - val_loss: 1.1512 - val_acc: 0.5559\n","Epoch 75/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1242 - acc: 0.5775 - val_loss: 1.1565 - val_acc: 0.5570\n","Epoch 76/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1361 - acc: 0.5654 - val_loss: 1.1427 - val_acc: 0.5623\n","Epoch 77/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1340 - acc: 0.5720 - val_loss: 1.1471 - val_acc: 0.5692\n","Epoch 78/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1216 - acc: 0.5730 - val_loss: 1.1495 - val_acc: 0.5561\n","Epoch 79/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1257 - acc: 0.5742 - val_loss: 1.1572 - val_acc: 0.5645\n","Epoch 80/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1210 - acc: 0.5761 - val_loss: 1.1579 - val_acc: 0.5578\n","Epoch 81/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1163 - acc: 0.5765 - val_loss: 1.1605 - val_acc: 0.5548\n","Epoch 82/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1045 - acc: 0.5840 - val_loss: 1.1515 - val_acc: 0.5559\n","Epoch 83/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1032 - acc: 0.5831 - val_loss: 1.1385 - val_acc: 0.5723\n","Epoch 84/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1087 - acc: 0.5836 - val_loss: 1.1431 - val_acc: 0.5704\n","Epoch 85/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.1037 - acc: 0.5814 - val_loss: 1.1145 - val_acc: 0.5723\n","Epoch 86/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1050 - acc: 0.5845 - val_loss: 1.1269 - val_acc: 0.5793\n","Epoch 87/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1029 - acc: 0.5810 - val_loss: 1.1402 - val_acc: 0.5667\n","Epoch 88/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.1012 - acc: 0.5835 - val_loss: 1.1094 - val_acc: 0.5793\n","Epoch 89/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0905 - acc: 0.5875 - val_loss: 1.1584 - val_acc: 0.5645\n","Epoch 90/100\n","505/504 [==============================] - 25s 50ms/step - loss: 1.1038 - acc: 0.5817 - val_loss: 1.1203 - val_acc: 0.5720\n","Epoch 91/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.0971 - acc: 0.5857 - val_loss: 1.1246 - val_acc: 0.5726\n","Epoch 92/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0966 - acc: 0.5843 - val_loss: 1.1078 - val_acc: 0.5834\n","Epoch 93/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.0820 - acc: 0.5907 - val_loss: 1.1149 - val_acc: 0.5773\n","Epoch 94/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0943 - acc: 0.5888 - val_loss: 1.1001 - val_acc: 0.5784\n","Epoch 95/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.0831 - acc: 0.5894 - val_loss: 1.1082 - val_acc: 0.5795\n","Epoch 96/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0795 - acc: 0.5926 - val_loss: 1.1219 - val_acc: 0.5773\n","Epoch 97/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.0716 - acc: 0.5950 - val_loss: 1.1354 - val_acc: 0.5667\n","Epoch 98/100\n","505/504 [==============================] - 24s 48ms/step - loss: 1.0924 - acc: 0.5869 - val_loss: 1.1046 - val_acc: 0.5862\n","Epoch 99/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0657 - acc: 0.5988 - val_loss: 1.1260 - val_acc: 0.5690\n","Epoch 100/100\n","505/504 [==============================] - 25s 49ms/step - loss: 1.0869 - acc: 0.5891 - val_loss: 1.0912 - val_acc: 0.5946\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd890defac8>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"zs3eo1HWPLjF","colab_type":"code","colab":{}},"source":["model.save('Resnet18_model.hdf5')\n","model.save_weights(\"weights_100_epochs_Resnet18.hdf5\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpEjgbVianJn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e8ee555d-c8a0-4c07-c9c7-d416dcbb234a","executionInfo":{"status":"ok","timestamp":1575068829712,"user_tz":-120,"elapsed":809,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["model.summary()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Model: \"ResNet18\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 48, 48, 3)    0                                            \n","__________________________________________________________________________________________________\n","zero_padding2d_1 (ZeroPadding2D (None, 54, 54, 3)    0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv1 (Conv2D)                  (None, 24, 24, 64)   9472        zero_padding2d_1[0][0]           \n","__________________________________________________________________________________________________\n","bn_conv1 (BatchNormalization)   (None, 24, 24, 64)   256         conv1[0][0]                      \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 24, 24, 64)   0           bn_conv1[0][0]                   \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 11, 11, 64)   0           activation_1[0][0]               \n","__________________________________________________________________________________________________\n","res2a_branch2a (Conv2D)         (None, 11, 11, 64)   4160        max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","bn2a_branch2a (BatchNormalizati (None, 11, 11, 64)   256         res2a_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 11, 11, 64)   0           bn2a_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res2a_branch2b (Conv2D)         (None, 11, 11, 64)   36928       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","bn2a_branch2b (BatchNormalizati (None, 11, 11, 64)   256         res2a_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 11, 11, 64)   0           bn2a_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res2a_branch2c (Conv2D)         (None, 11, 11, 256)  16640       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","res2a_branch1 (Conv2D)          (None, 11, 11, 256)  16640       max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","bn2a_branch2c (BatchNormalizati (None, 11, 11, 256)  1024        res2a_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","bn2a_branch1 (BatchNormalizatio (None, 11, 11, 256)  1024        res2a_branch1[0][0]              \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 11, 11, 256)  0           bn2a_branch2c[0][0]              \n","                                                                 bn2a_branch1[0][0]               \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 11, 11, 256)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","res2b_branch2a (Conv2D)         (None, 11, 11, 64)   16448       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","bn2b_branch2a (BatchNormalizati (None, 11, 11, 64)   256         res2b_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 11, 11, 64)   0           bn2b_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res2b_branch2b (Conv2D)         (None, 11, 11, 64)   36928       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","bn2b_branch2b (BatchNormalizati (None, 11, 11, 64)   256         res2b_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 11, 11, 64)   0           bn2b_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res2b_branch2c (Conv2D)         (None, 11, 11, 256)  16640       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","bn2b_branch2c (BatchNormalizati (None, 11, 11, 256)  1024        res2b_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 11, 11, 256)  0           bn2b_branch2c[0][0]              \n","                                                                 activation_4[0][0]               \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 11, 11, 256)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","res3a_branch2a (Conv2D)         (None, 6, 6, 128)    32896       activation_7[0][0]               \n","__________________________________________________________________________________________________\n","bn3a_branch2a (BatchNormalizati (None, 6, 6, 128)    512         res3a_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 6, 6, 128)    0           bn3a_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res3a_branch2b (Conv2D)         (None, 6, 6, 128)    147584      activation_8[0][0]               \n","__________________________________________________________________________________________________\n","bn3a_branch2b (BatchNormalizati (None, 6, 6, 128)    512         res3a_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 6, 6, 128)    0           bn3a_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res3a_branch2c (Conv2D)         (None, 6, 6, 512)    66048       activation_9[0][0]               \n","__________________________________________________________________________________________________\n","res3a_branch1 (Conv2D)          (None, 6, 6, 512)    131584      activation_7[0][0]               \n","__________________________________________________________________________________________________\n","bn3a_branch2c (BatchNormalizati (None, 6, 6, 512)    2048        res3a_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","bn3a_branch1 (BatchNormalizatio (None, 6, 6, 512)    2048        res3a_branch1[0][0]              \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 6, 6, 512)    0           bn3a_branch2c[0][0]              \n","                                                                 bn3a_branch1[0][0]               \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 6, 6, 512)    0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","res3b_branch2a (Conv2D)         (None, 6, 6, 128)    65664       activation_10[0][0]              \n","__________________________________________________________________________________________________\n","bn3b_branch2a (BatchNormalizati (None, 6, 6, 128)    512         res3b_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 6, 6, 128)    0           bn3b_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res3b_branch2b (Conv2D)         (None, 6, 6, 128)    147584      activation_11[0][0]              \n","__________________________________________________________________________________________________\n","bn3b_branch2b (BatchNormalizati (None, 6, 6, 128)    512         res3b_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, 6, 6, 128)    0           bn3b_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res3b_branch2c (Conv2D)         (None, 6, 6, 512)    66048       activation_12[0][0]              \n","__________________________________________________________________________________________________\n","bn3b_branch2c (BatchNormalizati (None, 6, 6, 512)    2048        res3b_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 6, 6, 512)    0           bn3b_branch2c[0][0]              \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, 6, 6, 512)    0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","res4a_branch2a (Conv2D)         (None, 3, 3, 256)    131328      activation_13[0][0]              \n","__________________________________________________________________________________________________\n","bn4a_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4a_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, 3, 3, 256)    0           bn4a_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res4a_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_14[0][0]              \n","__________________________________________________________________________________________________\n","bn4a_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4a_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, 3, 3, 256)    0           bn4a_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res4a_branch2c (Conv2D)         (None, 3, 3, 1024)   263168      activation_15[0][0]              \n","__________________________________________________________________________________________________\n","res4a_branch1 (Conv2D)          (None, 3, 3, 1024)   525312      activation_13[0][0]              \n","__________________________________________________________________________________________________\n","bn4a_branch2c (BatchNormalizati (None, 3, 3, 1024)   4096        res4a_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","bn4a_branch1 (BatchNormalizatio (None, 3, 3, 1024)   4096        res4a_branch1[0][0]              \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 3, 3, 1024)   0           bn4a_branch2c[0][0]              \n","                                                                 bn4a_branch1[0][0]               \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, 3, 3, 1024)   0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","res4b_branch2a (Conv2D)         (None, 3, 3, 256)    262400      activation_16[0][0]              \n","__________________________________________________________________________________________________\n","bn4b_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4b_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, 3, 3, 256)    0           bn4b_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res4b_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_17[0][0]              \n","__________________________________________________________________________________________________\n","bn4b_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4b_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, 3, 3, 256)    0           bn4b_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res4b_branch2c (Conv2D)         (None, 3, 3, 1024)   263168      activation_18[0][0]              \n","__________________________________________________________________________________________________\n","bn4b_branch2c (BatchNormalizati (None, 3, 3, 1024)   4096        res4b_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 3, 3, 1024)   0           bn4b_branch2c[0][0]              \n","                                                                 activation_16[0][0]              \n","__________________________________________________________________________________________________\n","activation_19 (Activation)      (None, 3, 3, 1024)   0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","res5a_branch2a (Conv2D)         (None, 2, 2, 512)    524800      activation_19[0][0]              \n","__________________________________________________________________________________________________\n","bn5a_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_20 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res5a_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_20[0][0]              \n","__________________________________________________________________________________________________\n","bn5a_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_21 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res5a_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_21[0][0]              \n","__________________________________________________________________________________________________\n","res5a_branch1 (Conv2D)          (None, 2, 2, 2048)   2099200     activation_19[0][0]              \n","__________________________________________________________________________________________________\n","bn5a_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5a_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","bn5a_branch1 (BatchNormalizatio (None, 2, 2, 2048)   8192        res5a_branch1[0][0]              \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 2, 2, 2048)   0           bn5a_branch2c[0][0]              \n","                                                                 bn5a_branch1[0][0]               \n","__________________________________________________________________________________________________\n","activation_22 (Activation)      (None, 2, 2, 2048)   0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","res5b_branch2a (Conv2D)         (None, 2, 2, 512)    1049088     activation_22[0][0]              \n","__________________________________________________________________________________________________\n","bn5b_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2a[0][0]             \n","__________________________________________________________________________________________________\n","activation_23 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2a[0][0]              \n","__________________________________________________________________________________________________\n","res5b_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_23[0][0]              \n","__________________________________________________________________________________________________\n","bn5b_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2b[0][0]             \n","__________________________________________________________________________________________________\n","activation_24 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2b[0][0]              \n","__________________________________________________________________________________________________\n","res5b_branch2c (Conv2D)         (None, 2, 2, 2048)   1050624     activation_24[0][0]              \n","__________________________________________________________________________________________________\n","bn5b_branch2c (BatchNormalizati (None, 2, 2, 2048)   8192        res5b_branch2c[0][0]             \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 2, 2, 2048)   0           bn5b_branch2c[0][0]              \n","                                                                 activation_22[0][0]              \n","__________________________________________________________________________________________________\n","activation_25 (Activation)      (None, 2, 2, 2048)   0           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","average_pooling2d_1 (AveragePoo (None, 1, 1, 2048)   0           activation_25[0][0]              \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 2048)         0           average_pooling2d_1[0][0]        \n","__________________________________________________________________________________________________\n","fc7 (Dense)                     (None, 7)            14343       flatten_1[0][0]                  \n","==================================================================================================\n","Total params: 14,006,791\n","Trainable params: 13,975,943\n","Non-trainable params: 30,848\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gtp3RsJSawPH","colab_type":"code","colab":{}},"source":["data2 = pd.read_csv('/content/fer2013/fer2013.csv')\n","data2 = data2.loc[ data2['emotion'].isin([0,3,4,6])]\n","data2.loc[(data2['emotion'] == 3),'emotion'] = 1\n","data2.loc[(data2['emotion'] == 4),'emotion'] = 2\n","data2.loc[(data2['emotion'] == 6),'emotion'] = 3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4S61EZREeT2x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"295c7c9c-9dee-4ba2-d24d-8ac4707a3338","executionInfo":{"status":"ok","timestamp":1575069952363,"user_tz":-120,"elapsed":28328,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["train = data2.loc[ data2['Usage'] == \"Training\"]\n","test = data2[data2.Usage == \"PrivateTest\"]\n","test_pub = data2[data2.Usage == \"PublicTest\"]\n","\n","train_labels = train[\"emotion\"].values\n","test_labels = test[\"emotion\"].values\n","test_pub_labels = test_pub[\"emotion\"].values\n","\n","train_ds = train[\"pixels\"].values\n","test_ds = test[\"pixels\"].values\n","test_pub_ds = test_pub[\"pixels\"].values\n","\n","train_ds_list = list()\n","test_ds_list = list()\n","test_pub_ds_list = list()\n","print(train_ds.size)\n","print(test_ds.size)\n","print(test_pub_ds.size)\n","for i in range(train_ds.size):\n","  train_ds_list.append(np.array(train_ds[i].split(\" \")))\n","  train_ds_list[i] = train_ds_list[i].astype(np.float)\n","\n","for i in range(test_ds.size):\n","  test_ds_list.append(np.array(test_ds[i].split(\" \")))\n","  test_ds_list[i] = test_ds_list[i].astype(np.float)\n","\n","for i in range(test_pub_ds.size):\n","  test_pub_ds_list.append(np.array(test_pub_ds[i].split(\" \")))\n","  test_pub_ds_list[i] = test_pub_ds_list[i].astype(np.float)\n","\n","train_np = np.array(train_ds_list)\n","test_np = np.array(test_ds_list)\n","test_pub_np = np.array(test_pub_ds_list)\n","\n","\n","\n","train_np = train_np.reshape(train_np.shape[0], 48, 48, 1)\n","test_np = test_np.reshape(test_np.shape[0],48, 48, 1)\n","test_pub_np = test_pub_np.reshape(test_pub_np.shape[0], 48, 48, 1)\n","\n","\n","train_np = np.repeat(train_np , 3, axis=3)\n","test_np = np.repeat(test_np , 3, axis=3)\n","test_pub_np = np.repeat(test_pub_np , 3, axis=3)\n","\n","\n","train_labels = np_utils.to_categorical(train_labels, 4)\n","test_labels = np_utils.to_categorical(test_labels, 4)\n","test_pub_labels = np_utils.to_categorical(test_pub_labels, 4) \n","\n","patience = 50\n","batch_size = 64\n","num_epochs = 60\n","\n","\n","train_big = np.vstack([train_np, test_pub_np])\n","train_label_big = np.concatenate([train_labels, test_pub_labels])"],"execution_count":22,"outputs":[{"output_type":"stream","text":["21005\n","2590\n","2622\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8gflxOpCewe7","colab_type":"code","colab":{}},"source":["data_generator = ImageDataGenerator(rescale = 1./255.,\n","                                   rotation_range = 40,\n","                                   width_shift_range = 0.2,\n","                                   height_shift_range = 0.2,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKrLGaM7fDa4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3c108d98-a6a8-4166-9d58-32895f9967c2","executionInfo":{"status":"ok","timestamp":1575072833223,"user_tz":-120,"elapsed":2041068,"user":{"displayName":"Mohamed Okasha","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2VKoerv3j_Rtfe7zg0cXnLoRNK4k7p0AqiD1D=s64","userId":"11296447201723808936"}}},"source":["model = ResNet18(input_shape = (48, 48, 3), classes = 4)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","model.fit_generator(data_generator.flow(train_big, train_label_big),\n","                        steps_per_epoch=len(train_big) / batch_size,\n","                        epochs=100, verbose=1, \n","                    validation_data=data_generator.flow(test_np, test_labels)\n",")"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","370/369 [==============================] - 30s 82ms/step - loss: 2.0457 - acc: 0.2922 - val_loss: 1.7743 - val_acc: 0.3371\n","Epoch 2/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.7173 - acc: 0.3185 - val_loss: 1.8077 - val_acc: 0.3066\n","Epoch 3/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.6454 - acc: 0.3331 - val_loss: 1.4458 - val_acc: 0.3375\n","Epoch 4/100\n","370/369 [==============================] - 21s 57ms/step - loss: 1.4516 - acc: 0.3422 - val_loss: 1.7034 - val_acc: 0.3301\n","Epoch 5/100\n","370/369 [==============================] - 21s 57ms/step - loss: 1.4860 - acc: 0.3411 - val_loss: 1.4251 - val_acc: 0.3347\n","Epoch 6/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4205 - acc: 0.3454 - val_loss: 1.4837 - val_acc: 0.3425\n","Epoch 7/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4113 - acc: 0.3415 - val_loss: 1.3876 - val_acc: 0.3425\n","Epoch 8/100\n","370/369 [==============================] - 21s 57ms/step - loss: 1.3988 - acc: 0.3450 - val_loss: 1.4562 - val_acc: 0.3583\n","Epoch 9/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4204 - acc: 0.3403 - val_loss: 1.3889 - val_acc: 0.3467\n","Epoch 10/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4348 - acc: 0.3416 - val_loss: 1.3905 - val_acc: 0.3413\n","Epoch 11/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4036 - acc: 0.3540 - val_loss: 1.3995 - val_acc: 0.3568\n","Epoch 12/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4009 - acc: 0.3491 - val_loss: 1.3793 - val_acc: 0.3471\n","Epoch 13/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3965 - acc: 0.3466 - val_loss: 1.3953 - val_acc: 0.3463\n","Epoch 14/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4062 - acc: 0.3537 - val_loss: 1.3815 - val_acc: 0.3247\n","Epoch 15/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3980 - acc: 0.3446 - val_loss: 1.3613 - val_acc: 0.3456\n","Epoch 16/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.4002 - acc: 0.3458 - val_loss: 1.4474 - val_acc: 0.3529\n","Epoch 17/100\n","370/369 [==============================] - 21s 57ms/step - loss: 1.3734 - acc: 0.3570 - val_loss: 1.3414 - val_acc: 0.3548\n","Epoch 18/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3766 - acc: 0.3600 - val_loss: 1.3819 - val_acc: 0.3722\n","Epoch 19/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.3684 - acc: 0.3685 - val_loss: 1.3582 - val_acc: 0.3772\n","Epoch 20/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.3618 - acc: 0.3723 - val_loss: 1.3703 - val_acc: 0.3664\n","Epoch 21/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3625 - acc: 0.3709 - val_loss: 1.3583 - val_acc: 0.3668\n","Epoch 22/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3578 - acc: 0.3742 - val_loss: 1.4593 - val_acc: 0.3656\n","Epoch 23/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3795 - acc: 0.3718 - val_loss: 1.3362 - val_acc: 0.3707\n","Epoch 24/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3559 - acc: 0.3713 - val_loss: 1.3463 - val_acc: 0.3707\n","Epoch 25/100\n","370/369 [==============================] - 21s 57ms/step - loss: 1.3407 - acc: 0.3789 - val_loss: 1.3726 - val_acc: 0.3776\n","Epoch 26/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3153 - acc: 0.3937 - val_loss: 1.3367 - val_acc: 0.3888\n","Epoch 27/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.3010 - acc: 0.4094 - val_loss: 1.3227 - val_acc: 0.3965\n","Epoch 28/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.3070 - acc: 0.3963 - val_loss: 1.3440 - val_acc: 0.3888\n","Epoch 29/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.2815 - acc: 0.4113 - val_loss: 1.2655 - val_acc: 0.4297\n","Epoch 30/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.2743 - acc: 0.4205 - val_loss: 1.2223 - val_acc: 0.4541\n","Epoch 31/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.2318 - acc: 0.4416 - val_loss: 1.2147 - val_acc: 0.4606\n","Epoch 32/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.2443 - acc: 0.4411 - val_loss: 1.2227 - val_acc: 0.4239\n","Epoch 33/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.2563 - acc: 0.4313 - val_loss: 1.2056 - val_acc: 0.4602\n","Epoch 34/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.2124 - acc: 0.4552 - val_loss: 1.2024 - val_acc: 0.4533\n","Epoch 35/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.1843 - acc: 0.4672 - val_loss: 1.1803 - val_acc: 0.4757\n","Epoch 36/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.1924 - acc: 0.4593 - val_loss: 1.1804 - val_acc: 0.4753\n","Epoch 37/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.2184 - acc: 0.4555 - val_loss: 1.1842 - val_acc: 0.4707\n","Epoch 38/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.1824 - acc: 0.4703 - val_loss: 1.1684 - val_acc: 0.4799\n","Epoch 39/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.1746 - acc: 0.4815 - val_loss: 1.2128 - val_acc: 0.4552\n","Epoch 40/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.1499 - acc: 0.4876 - val_loss: 1.1214 - val_acc: 0.5027\n","Epoch 41/100\n","370/369 [==============================] - 20s 54ms/step - loss: 1.1684 - acc: 0.4828 - val_loss: 1.1413 - val_acc: 0.4915\n","Epoch 42/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.1403 - acc: 0.4995 - val_loss: 1.1350 - val_acc: 0.5027\n","Epoch 43/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.1443 - acc: 0.4958 - val_loss: 1.1086 - val_acc: 0.5216\n","Epoch 44/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.1144 - acc: 0.5078 - val_loss: 1.0846 - val_acc: 0.5139\n","Epoch 45/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.1035 - acc: 0.5122 - val_loss: 1.0811 - val_acc: 0.5251\n","Epoch 46/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.0907 - acc: 0.5183 - val_loss: 1.0648 - val_acc: 0.5340\n","Epoch 47/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.0889 - acc: 0.5245 - val_loss: 1.0586 - val_acc: 0.5452\n","Epoch 48/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.0701 - acc: 0.5352 - val_loss: 1.0538 - val_acc: 0.5390\n","Epoch 49/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.0734 - acc: 0.5345 - val_loss: 1.0536 - val_acc: 0.5390\n","Epoch 50/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.0446 - acc: 0.5512 - val_loss: 1.0232 - val_acc: 0.5463\n","Epoch 51/100\n","370/369 [==============================] - 20s 54ms/step - loss: 1.0278 - acc: 0.5487 - val_loss: 1.0206 - val_acc: 0.5517\n","Epoch 52/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.0190 - acc: 0.5588 - val_loss: 1.0090 - val_acc: 0.5525\n","Epoch 53/100\n","370/369 [==============================] - 21s 55ms/step - loss: 1.0093 - acc: 0.5655 - val_loss: 0.9941 - val_acc: 0.5799\n","Epoch 54/100\n","370/369 [==============================] - 21s 56ms/step - loss: 1.0153 - acc: 0.5596 - val_loss: 1.0165 - val_acc: 0.5641\n","Epoch 55/100\n","370/369 [==============================] - 20s 55ms/step - loss: 1.0014 - acc: 0.5735 - val_loss: 0.9662 - val_acc: 0.5892\n","Epoch 56/100\n","370/369 [==============================] - 21s 56ms/step - loss: 0.9923 - acc: 0.5736 - val_loss: 0.9905 - val_acc: 0.5749\n","Epoch 57/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.9854 - acc: 0.5799 - val_loss: 0.9722 - val_acc: 0.5842\n","Epoch 58/100\n","370/369 [==============================] - 21s 56ms/step - loss: 0.9828 - acc: 0.5819 - val_loss: 0.9695 - val_acc: 0.5888\n","Epoch 59/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9692 - acc: 0.5868 - val_loss: 0.9720 - val_acc: 0.5873\n","Epoch 60/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.9696 - acc: 0.5910 - val_loss: 0.9407 - val_acc: 0.5892\n","Epoch 61/100\n","370/369 [==============================] - 21s 55ms/step - loss: 0.9494 - acc: 0.5998 - val_loss: 0.9488 - val_acc: 0.6004\n","Epoch 62/100\n","370/369 [==============================] - 21s 56ms/step - loss: 0.9643 - acc: 0.5916 - val_loss: 0.9486 - val_acc: 0.5873\n","Epoch 63/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9523 - acc: 0.5919 - val_loss: 0.9413 - val_acc: 0.6112\n","Epoch 64/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.9340 - acc: 0.6039 - val_loss: 0.9310 - val_acc: 0.5954\n","Epoch 65/100\n","370/369 [==============================] - 21s 56ms/step - loss: 0.9381 - acc: 0.6062 - val_loss: 0.9515 - val_acc: 0.5969\n","Epoch 66/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.9374 - acc: 0.6032 - val_loss: 0.9175 - val_acc: 0.6185\n","Epoch 67/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9394 - acc: 0.6059 - val_loss: 0.9369 - val_acc: 0.6054\n","Epoch 68/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.9257 - acc: 0.6031 - val_loss: 0.9100 - val_acc: 0.6193\n","Epoch 69/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.9252 - acc: 0.6096 - val_loss: 0.9234 - val_acc: 0.6062\n","Epoch 70/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9169 - acc: 0.6174 - val_loss: 0.9134 - val_acc: 0.6139\n","Epoch 71/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9072 - acc: 0.6213 - val_loss: 0.8959 - val_acc: 0.6201\n","Epoch 72/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.9130 - acc: 0.6140 - val_loss: 0.8919 - val_acc: 0.6239\n","Epoch 73/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.8887 - acc: 0.6264 - val_loss: 0.9133 - val_acc: 0.6139\n","Epoch 74/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.9073 - acc: 0.6154 - val_loss: 0.8700 - val_acc: 0.6390\n","Epoch 75/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.8960 - acc: 0.6245 - val_loss: 0.8923 - val_acc: 0.6255\n","Epoch 76/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.8989 - acc: 0.6196 - val_loss: 0.8954 - val_acc: 0.6239\n","Epoch 77/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8910 - acc: 0.6255 - val_loss: 0.8861 - val_acc: 0.6247\n","Epoch 78/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.8860 - acc: 0.6240 - val_loss: 0.8765 - val_acc: 0.6293\n","Epoch 79/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8840 - acc: 0.6270 - val_loss: 0.8915 - val_acc: 0.6347\n","Epoch 80/100\n","370/369 [==============================] - 20s 55ms/step - loss: 0.8780 - acc: 0.6331 - val_loss: 0.8773 - val_acc: 0.6332\n","Epoch 81/100\n","370/369 [==============================] - 19s 52ms/step - loss: 0.8914 - acc: 0.6319 - val_loss: 0.8737 - val_acc: 0.6340\n","Epoch 82/100\n","370/369 [==============================] - 19s 53ms/step - loss: 0.8699 - acc: 0.6374 - val_loss: 0.8759 - val_acc: 0.6382\n","Epoch 83/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8710 - acc: 0.6379 - val_loss: 0.8668 - val_acc: 0.6282\n","Epoch 84/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8763 - acc: 0.6343 - val_loss: 0.8548 - val_acc: 0.6432\n","Epoch 85/100\n","370/369 [==============================] - 19s 53ms/step - loss: 0.8693 - acc: 0.6390 - val_loss: 0.8825 - val_acc: 0.6278\n","Epoch 86/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8561 - acc: 0.6452 - val_loss: 0.8607 - val_acc: 0.6266\n","Epoch 87/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8613 - acc: 0.6408 - val_loss: 0.8688 - val_acc: 0.6382\n","Epoch 88/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8569 - acc: 0.6420 - val_loss: 0.8538 - val_acc: 0.6452\n","Epoch 89/100\n","370/369 [==============================] - 19s 53ms/step - loss: 0.8512 - acc: 0.6446 - val_loss: 0.8630 - val_acc: 0.6398\n","Epoch 90/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8677 - acc: 0.6399 - val_loss: 0.8635 - val_acc: 0.6421\n","Epoch 91/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8517 - acc: 0.6483 - val_loss: 0.8378 - val_acc: 0.6417\n","Epoch 92/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8485 - acc: 0.6488 - val_loss: 0.8242 - val_acc: 0.6656\n","Epoch 93/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8535 - acc: 0.6476 - val_loss: 0.8595 - val_acc: 0.6324\n","Epoch 94/100\n","370/369 [==============================] - 19s 52ms/step - loss: 0.8355 - acc: 0.6530 - val_loss: 0.8439 - val_acc: 0.6486\n","Epoch 95/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8432 - acc: 0.6529 - val_loss: 0.8478 - val_acc: 0.6475\n","Epoch 96/100\n","370/369 [==============================] - 20s 54ms/step - loss: 0.8421 - acc: 0.6505 - val_loss: 0.8444 - val_acc: 0.6514\n","Epoch 97/100\n","370/369 [==============================] - 19s 52ms/step - loss: 0.8386 - acc: 0.6497 - val_loss: 0.8297 - val_acc: 0.6548\n","Epoch 98/100\n","370/369 [==============================] - 19s 52ms/step - loss: 0.8416 - acc: 0.6534 - val_loss: 0.8391 - val_acc: 0.6510\n","Epoch 99/100\n","370/369 [==============================] - 20s 53ms/step - loss: 0.8247 - acc: 0.6561 - val_loss: 0.8229 - val_acc: 0.6633\n","Epoch 100/100\n","370/369 [==============================] - 21s 56ms/step - loss: 0.8404 - acc: 0.6522 - val_loss: 0.8399 - val_acc: 0.6564\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd805ae5a58>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"cSVyX8TUfQ4j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}